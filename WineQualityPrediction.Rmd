---
title: "Wine Quality Prediction"
author: "Amine Agrane & Lydia Khelfane"
date: "10/02/2021"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---


```{r include=FALSE}
# import all the necessary library
library(caret)
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(randomForest)
```

# Project and Dataset Presentation      
In this notebook we will use the data from the Kaggle Repository (https://www.kaggle.com/rajyellow46/wine-quality) by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. The data include examples of red and white wines from Portugal-one of the world’s leading wine-producing countries. 

For each wine, a laboratory analysis measured characteristics such as acidity, sugar content, chlorides, sulfur, alcohol, pH, and density. The samples were then rated in a blind tasting by panels of no less than three judges on a quality scale ranging from zero (very bad) to 10 (excellent). In the case of judges disagreeing on the rating, the median value was used.

##### *Objective of this notebook*



The objective that we want to achieve through this notebook is to build a machine learning model of classification type, that will predict if a wine is considered as good or not. The model takes as input some wine characteristics (alcohol content, acidity, sugar proportion, etc), and gives as output a binary variable that describes the quality of the wine ("Good" or "Bad"). We'll use a decision tree as our classification model. We'll then use Random Forest to improve our classification scores.


##### *Used libraries*



The execution of this notebook needs the following library : 

 - caret
 - forcats
 - ggplot2
 - rpart
 - rpart.plot
 - randomForest

##### *Dataset description*
The wine dataset is composed by a total of 13 different variables : 

**1- fixed acidity (tartaric acid - g/dm3)** : most acids involved with wine or fixed or nonvolatile (do not evaporate readily)

**2- volatile acidity (acetic acid - g/dm3)**: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste

**3- citric acid (g/dm3) **: found in small quantities, citric acid can add ‘freshness’ and flavor to wines 

**4- residual sugar (g/dm3)** : the amount of sugar remaining after fermentation stops, it’s rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet

**5- chlorides (sodium chloride - g/dm3) **: the amount of salt in the wine

**6- free sulfur dioxide (mg/dm3) **: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine

**7- total sulfur dioxide (mg/dm3) **: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine

**8- density (g/cm3)** : the density of water is close to that of water depending on the percent alcohol and sugar content

**9- pH** : describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale

**10- sulphates (potassium sulphate - g/dm3)** : a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant

**11- alcohol (% by volume)** : the percent alcohol content of the wine Output variable (based on sensory data):

**12- quality**: score between 0 and 10 that describes the quality of the wine.

**13- color**: color of the wine. There are two type wine, red wine and white wine.

## Exploratory Data Analysis on the wine dataset
##### *Load the wine dataset*

We start by reading the data which is stored in the csv file `winequality.csv`. The file is loaded using the `read.csv` command, along with the `as.data.frame` command which store our data inside a structure of dataframe type.

```{r echo=TRUE}
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
```


## Basic analysis on the dataset
Now we're gonna achieve some basic analysis on our wine dataset to get a better understanding of its contents, size and structure.

**Dimension of our wine Dataset :**
```{r echo=TRUE}
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
```

**Structure and distribution of the variables :**
  
We use the `str` and `summary` functions to display some basic statistics about our dataset. The `str` function shows the nature (type) of each variable (feature) of our dateset, and some values that the feature can take. The `summary` function in other hand give us some statistics metrics (min, max, median, etc) for each feature of the dataset.
```{r echo=TRUE}
str(df_wine)
summary(df_wine)
```
**Correlation between the variables :**
```{r}
as.data.frame( cor(df_wine[c(-13, -14)]))
```


**Number of NAN values for each variables :**
```{r}
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
```



# Preparing the data
We want to predict the quality of the wine (score between 1 and 10) by using the rest of the feature variables. Here, wine quality is the variable we want to predict (Y variable).

##### Inspect the wine quality score feature

Let's take a look at the distribution of the wine quality score in our dataset. We see that Wine quality appear to follow a bell shaped distribution (Gaussian/Normal distribution). This implies most wines are of average quality and few are good or bad

```{r}
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution", ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
```

Now let's check the frequency of each wine quality score. From the above table, we see that an approximate 45% of the wine quality scores have a 6 score value.

```{r}
table(df_wine$quality)
```



##### Binning the Wine Quality Score Variable

We want to predict the quality of the wine based on the rest of the variables present in our dataset. We see from the above table that the wine quality score variable takes 7 different values (3, 4, 5, 6, 7, 8, 9) inside our dataset. If we want to build a machine learning model to predict this variable, we'll use a classification model that will achieve a multiclass classification, in our case we have 7 distinct classes (7 possible values).

The question we want to answer through this notebook is whatever a wine is good or bad ? In our case, the output variable (Y) that we want to get is a binary variable 
True => "Good" and False => "Bad". 

We transform our win quality score variable to a binary variable by achieving a binning on its values, i.e we fix a specific threshold and associate each score to a unique class, the class "Bad" for the lower quality wines and the class "Good" for the best wines. 

Threshold :
  - Good : scores from 7 to 9.
  - Bad : scores from 1 to 6.


```{r}
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
```


```{r}
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
```


# Build a Decision Tree Model to predict wine quality class. 
##### *Splitting the data into training and testing sets*
We split our data into two different sets : 

  - Training set for the model fitting (learning the patterns)
  - Testing set for estimating the model’s accuracy


```{r}
# we set the seed to get reproducible results
set.seed(10)

# get the training dataset indexes
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)

# split into train and test sets
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
```


##### Instantiate and train the Decision Tree Model   
 
We will begin by training a classification tree model. Although almost any implementation of decision trees can be used to perform classification tree modeling, the `rpart` (recursive partitioning) package offers the most faithful implementation of classification trees as they were described by the CART team. As the classic R implementation of CART, the rpart package is also well-documented and supported with functions for visualizing and evaluating the rpart models.

Using the R formula interface, we can specify `qualityClass` as the outcome variable and use the dot notation to allow all the other columns in the training_data data frame to be used as predictors. 

```{r}
rpart_model <- rpart(qualityClass~., data=training_data, method="class")

# choosing the best complexity parameter "cp" to prune the tree
cp.optim <- rpart_model$cptable[which.min(rpart_model$cptable[,"xerror"]),"CP"]

# tree prunning using the best complexity parameter. For more in
tree <- prune(rpart_model, cp=cp.optim)
```


##### Visualization of the decision tree model

The tree can be understood using only the preceding output, it is often more readable using visualization. After installing the package using the install.packages(“rpart.plot”) command, the rpart.plot() function produces a tree diagram from any rpart model object. The following commands plot the classification tree we built earlier which produces a tree diagram is as follows:

For each node in the tree, the number of examples reaching the decision point is listed. For instance, all 5198 examples (100% of the examples) begin at the root node, of which 64% have alcohol < 11. Because alcohol was used first in the tree, it is the single most important predictor of wine quality class.

```{r}
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
```



# Model Evaluation
The evaluation of machine learning algorithms consists in comparing the predicted value with the actual outcome. The confusion matrix displays the predicted and observed values in a table and provides additional statistics summary. We'll the `confusionMatrix` function to display the following metric about our classification model :

  - **The confusion matrix** : The confusion matrix is a simple table with the cross tabulation of the   predicted values with the actual observed values. All values are in absolute numbers of observations and predictions, so for example the True Positive is the number of predicted values that are exactly the same as the actual values. In our case, a True Positive is a Bad wine (in terms of quality) that was correctly classified as Bad.
 
  - **Sensitivity** :  It's the proportion of positive values when they are actually positive, i.e. the ability to predict positive values.
  
  
  - **Specificity** : It's the probability of a predicted negative value conditioned to a negative outcome.
  

```{r}
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confusionMatrix(predict_rpart, test_data[, 13])
```
Using the `confusionMatrix` function on our decision tree model, we get the following results :

 - Accuracy : 0.8129
 - Sensitivity : 0.9601 
 - Specificity : 0.2546 
 
The overall accuracy score is pretty good, but we have a low Specificity score which mean that we globally fail to detect a True Negative (in our case a True Negative -> Correctly classified as Good wine). In order to improve classification score, we're gonna use in the next section a more complex classification model, which is a Random Forest classifier.


# Improve model Accuracy by using a Random Forest
While decision trees are easy to interpret, they tend to be rather simplistic and are often outperformed by other algorithms. Random Forests are one way to improve the performance of decision trees. The algorithm starts by building out trees similar to the way a normal decision tree algorithm works. However, every time a split has to made, it uses only a small random subset of features to make the split instead of the full set of features.

From the below output, we instantiate and train a random forest model on our wine dataset. This model will generates 1000  decision trees with 3 variables randomly sampled (approximate of sqrt(number of variables) = 4) used for splitting.
  
```{r}
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=4)
rf_model
```
##### Evaluate the Random Forest model
Same as for the decision tree model,  we use the `confusionMatrix` function to evaluate the performances of our Random Forest Classifier. This step is to check how well the model performs with the test data. We see that we now have a better classification model, as the evaluation metrics have all been improved.  From the below confusion matrix, we get the following results for our Random Forest model :

 - Accuracy : 0.8876 
 - Sensitivity : 0.9708 
 - Specificity : 0.5720 

998 wines are classified as low-quality wines and 155 are high-quality wines. The accuracy of the model is 88.5%. That means 11.75% (116+30) of wines are predicted wrong by the model. If a customer picks a high-quality wine there is only 15% (30/(30+155)) chance that wine is low quality wine. In the Random Forest model, the prediction of high-quality wines is more accurate compared to a single decision tree model.


 (total low-quality wines predicted by low-quality wines) is the percentage of predicted low-quality wines 97% which is good whereas the specificity gives the percentage of predicted high-quality wines i.e. only 57%. This could be explained because the model may be biased in classifying more low-quality wines due to an unbalanced wine quality dataset.
```{r}
rf_predict <- predict(rf_model,  test_data[, -13])
confusionMatrix(rf_predict, test_data[, 13])
```
##### Identifying Important Variables
In this section, we're gonna use the `varImpPlot` function that return a plot which describes the importance of the dataset variable in taking a decision (classify a wine as Good or Bad). The measure of importance of a variable is taken as the mean decrease in the Gini coefficient, which is a measure of statistical dispersion.

The variables which are highly important in determining the high-quality wine are ordered top-to-bottom with most-to-least important in the below plot. We from the plot that alcohol is a very important variable with more than 250 as a mean decrease in Gini followed by density and residual.acidity  for the classification of wines.

In random forest models, alcohol is an important variable in deciding the quality of the wine. If we know what to look for at the labels before purchasing wine, there is a high chance of picking the right quality wine. The accuracy of predicting high-quality wines, using Random Forest, wine quality is 88.5%%. From the confusion matrix, 3% (35) of low-quality wines are predicted as high-quality type. If a customer wants to enjoy a high-quality wine, the chances of picking high-quality wine are high.

```{r}
varImpPlot(rf_model)
```




# Conclusion
The objective of this notebook was to build a classification model able to predict the quality class of a given wine (Good or Bad) based on its characteristics. After a first step of exploratory data analysis that helped us to better understand the data we're working on, we performed some transformations on our data to facilitate the construction of classification model. After training a first decision tree model, we we're not totally satisfied with the model performances, so we decided to try another more complex classification model which is a Random Forest Classifier. 

Prediction of high-quality wines with Random Forest model is good with less overfitting problems and with high accuracy (from step4); because of the aggregation of decision trees. To pick a desired quality wine, it is always important to check few parameters (from step5) and it’s value ranges to know whether we picked the right one or not. Overall, the correct prediction of wines is necessary because there would be chances that the wine we have picked would be not our interest.
