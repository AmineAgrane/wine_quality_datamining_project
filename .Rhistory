library(tree)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
df_wine <- subset (df_wine, select = -good)
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold 1 distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
cat('Threshold 2 distribution :', table(ifelse(df_wine$quality<=5,'Bad', ifelse(df_wine$quality>=6, "Good",""))))
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# split the datasets
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
tree_model <- tree(qualityClass~., data=training_data)
predict_tree <- predict(tree_model, test_data[, -13], type="class")
mean(predict_tree != test_data[, 13])
plot(tree_model)
text(tree_model, pretyy=0)
mean(predict_tree != test_data[, 13])
plot(tree_model)
text(tree_model, pretty=0)
# import all the library
library(forcats)
library(ggplot2)
library(tree)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
df_wine <- subset (df_wine, select = -good)
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
View(df_wine)
View(df_wine)
# we set the seed to get reproducible results
set.seed(10)
# split the datasets
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
tree_model <- tree(qualityClass~., data=training_data)
predict_tree <- predict(tree_model, test_data[, -13], type="class")
mean(predict_tree != test_data[, 13])
plot(tree_model)
text(tree_model, pretty=0)
install.packages('rpart')
library(rpart)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
library(rpart)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
library(rpart)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
install.packages('rpart.plot')
library(rpart)
library(rpart.plot)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
# import all the library
library(forcats)
library(ggplot2)
library(tree)
# import all the library
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
View(df_wine)
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# split the datasets
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
mean(predict_tree != test_data[, 13])
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
#rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
#rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
#rpart.plot(rpart_model)
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
mean(predict_rpart != test_data[, 13])
rpart.plot(rpart_model)
# Visualise the model graphicaly
rpart.plot(rpart_model)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=2)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=1)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=3)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=9)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=100)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the model graphicaly
rpart.plot(rpart_model, extra=101)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
# Visualise the decision tree model graphically
#rpart.plot(rpart_model, extra=101)
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
# import all the library
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# get the training dataset indexes
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
# split into train and test sets
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
# import all the library
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# get the training dataset indexes
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
# split into train and test sets
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
#predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#mean(predict_rpart != test_data[, 13])
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
# choosing the best complexity parameter "cp" to prune the tree
cp.optim <- rpart_model$cptable[which.min(rpart_model$cptable[,"xerror"]),"CP"]
# tree prunning using the best complexity parameter. For more in
tree <- prune(rpart_model, cp=cp.optim)
# Visualise the decision tree model graphically
rpart.plot(tree, extra=101)
# Visualise the decision tree model graphically
rpart.plot(tree, extra=101)
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
t <- table(test_data[, 13])
confusionMatrix(t)
library(caret,quietly = TRUE)
install.packages('caret')
install.packages('quietly')
library(caret,quietly = TRUE)
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
t <- table(test_data[, 13], predict_rpart)
confusionMatrix(t)
install.packages('e1071')
library(caret,quietly = TRUE)
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
t <- table(test_data[, 13], predict_rpart)
confusionMatrix(t)
mean(predict_rpart != test_data[, 13])
install.packages("randomForest")
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=sqrt(p))
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=sqrt(p))
library(randomForest)
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=sqrt(p))
library(randomForest)
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000)
rf_model
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13], type="class")
#
rf_table <- table(test_data[, 13], predict_rpart)
confusionMatrix(rf_table)
# import all the library
library(caret)
library(quietly)
# import all the library
library(caret)
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(randomForest)
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13], type="class")
#
rf_table <- table(test_data[, 13], predict_rpart)
confusionMatrix(rf_table)
library(randomForest)
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=5)
rf_model
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13], type="class")
#
rf_table <- table(test_data[, 13], predict_rpart)
confusionMatrix(rf_table)
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
#
t <- table(test_data[, 13], predict_rpart)
confusionMatrix(t)
# store the predicted values by our decision tree model
#rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
tr.rfclass.prob = predict(rf_model, test_data[, -13])
confMat = confusionMatrix(tr.rfclass.prob, test.y, positive="1")
# store the predicted values by our decision tree model
#rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
tr.rfclass.prob = predict(rf_model, test_data[, -13])
confMat = confusionMatrix(tr.rfclass.prob, test_data[, 13], positive="1")
# store the predicted values by our decision tree model
#rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
tr.rfclass.prob = predict(rf_model, test_data[, -13])
confMat = confusionMatrix(tr.rfclass.prob, test_data[, 13], positive="1")
# store the predicted values by our decision tree model
#rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
tr.rfclass.prob = predict(rf_model, test_data[, -13])
confMat = confusionMatrix(tr.rfclass.prob, test_data[, 13])
tr.rfclass.eval = list(auc = NA, confusionMatrix = confMat);
tr.rfclass.eval
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
#tr.rfclass.prob = predict(rf_model, test_data[, -13])
confMat = confusionMatrix(rf_predict, test_data[, 13])
tr.rfclass.eval = list(auc = NA, confusionMatrix = confMat);
tr.rfclass.eval
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13])
#
#rf_table <- table(test_data[, 13], predict_rpart)
#confusionMatrix(rf_table)
confMat = confusionMatrix(rf_predict, test_data[, 13])
confMat
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confMatDT = confusionMatrix(predict_rpart, test_data[, 13])
confMatDT
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confMatDT = confusionMatrix(predict_rpart, test_data[, 13])
confMatDT
rf_model <- randomForest(qualityClass~., data=training_data, ntree=10000, mtry=6)
rf_model
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13])
confMat = confusionMatrix(rf_predict, test_data[, 13])
confMat
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=3)
rf_model
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13])
confMat = confusionMatrix(rf_predict, test_data[, 13])
confMat
len(df_wine)
size(df_wine)
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=4)
rf_model
# store the predicted values by our decision tree model
rf_predict <- predict(rf_model,  test_data[, -13])
confMat = confusionMatrix(rf_predict, test_data[, 13])
confMat
rf_predict <- predict(rf_model,  test_data[, -13])
confMat = confusionMatrix(rf_predict, test_data[, 13])
rf_predict <- predict(rf_model,  test_data[, -13])
confusionMatrix(rf_predict, test_data[, 13])
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confusionMatrix(predict_rpart, test_data[, 13])
head(confusionMatrix(rf_predict, test_data[, 13]))
confusionMatrix(rf_predict, test_data[, 13])$table
ggplot(data =  confusionMatrix(rf_predict, test_data[, 13]), mapping = aes(x = Bad, y = Good)) +
geom_tile(aes(fill = Y), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
scale_fill_gradient(low = "blue", high = "red") +
theme_bw() + theme(legend.position = "none")
View(df_wine)
# import all the necessary library
library(caret)
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(randomForest)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# get the training dataset indexes
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
# split into train and test sets
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
# choosing the best complexity parameter "cp" to prune the tree
cp.optim <- rpart_model$cptable[which.min(rpart_model$cptable[,"xerror"]),"CP"]
# tree prunning using the best complexity parameter. For more in
tree <- prune(rpart_model, cp=cp.optim)
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confusionMatrix(predict_rpart, test_data[, 13])
varImpPlot(rf_model)
# import all the necessary library
library(caret)
library(forcats)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(randomForest)
# Load the data
df_wine <- as.data.frame( read.csv(file = './data/winequality.csv', sep=',', stringsAsFactors=F))
head(df_wine)
tail(df_wine)
cat('The number of rows inside the dataset is : ', dim(df_wine)[1])
cat('The number of columns/features inside the dataset is : ', dim(df_wine)[2])
str(df_wine)
summary(df_wine)
as.data.frame( cor(df_wine[c(-13, -14)]))
cat('Number of NAN values for each variable :', colSums(is.na(df_wine)))
# Simple Bar Plot
hist(df_wine$quality, main="Wine Quality Score Distribution",ylab="Count", xlab="Wine Quality Score", breaks=10, , col= "darkgreen")
table(df_wine$quality)
cat('Threshold  distribution :', table(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, "Good",""))))
# We drop the quality column and replace it with the new qualityClass variable.
qualityClass <- as.factor(ifelse(df_wine$quality<=6,'Bad', ifelse(df_wine$quality>=7, 'Good','')))
df_wine <- data.frame(subset (df_wine, select = -quality), qualityClass)
# we set the seed to get reproducible results
set.seed(10)
# get the training dataset indexes
dataset_size <- dim(df_wine)[1]
train_set_size <- round(0.8*dataset_size)
train_index <- sample(dataset_size, train_set_size, replace=FALSE)
# split into train and test sets
training_data <- df_wine[train_index, ]
test_data <- df_wine[-train_index,]
rpart_model <- rpart(qualityClass~., data=training_data, method="class")
# choosing the best complexity parameter "cp" to prune the tree
cp.optim <- rpart_model$cptable[which.min(rpart_model$cptable[,"xerror"]),"CP"]
# tree prunning using the best complexity parameter. For more in
tree <- prune(rpart_model, cp=cp.optim)
# Visualise the decision tree model graphically
rpart.plot(rpart_model, extra=101)
# store the predicted values by our decision tree model
predict_rpart <- predict(rpart_model,  test_data[, -13], type="class")
confusionMatrix(predict_rpart, test_data[, 13])
rf_model <- randomForest(qualityClass~., data=training_data, ntree=1000, mtry=4)
rf_model
rf_predict <- predict(rf_model,  test_data[, -13])
confusionMatrix(rf_predict, test_data[, 13])
varImpPlot(rf_model)
